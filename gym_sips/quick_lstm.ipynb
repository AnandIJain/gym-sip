{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "save_path = './models/lstm.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileLoader:\n",
    "    def __init__(self, directory):\n",
    "        self.files = os.listdir(directory)\n",
    "        self.length = len(self.files)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        df = pd.read_csv(directory + files[index]) \n",
    "        return df.iloc[:, 1:5].values\n",
    "\n",
    "#         x = df.values #returns a numpy array\n",
    "#         min_max_scaler = preprocessing.MinMaxScaler()\n",
    "#         x_scaled = min_max_scaler.fit_transform(x)\n",
    "#         data = x_scaled\n",
    " \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "directory = \"/mnt/c/Users/Anand/home/Programming/datasets/price-volume-data-for-all-us-stocks-etfs/Data/Stocks/\"\n",
    "fileset = FileLoader(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLoader(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.samples = []\n",
    "        self.length = len(data)\n",
    "        self.window_len = 1\n",
    "        self.data = data\n",
    "        self.get_data()\n",
    "\n",
    "    def get_data(self):\n",
    "        for i in range(1, self.length - (self.window_len + 1)):\n",
    "            upper_idx = i + self.window_len\n",
    "            x = torch.tensor(self.data[i - 1:upper_idx - 1, :]).view(1, 1, -1).float()\n",
    "            y = torch.tensor(self.data[upper_idx, :]).view(1, 1, -1).float()\n",
    "            self.samples.append((x, y))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.length -  (self.window_len + 1)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.samples[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fileset[1]\n",
    "dataset = LSTMLoader(data)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "sample_x, sample_y = dataset[0]\n",
    "print(f'sample_x shape: {sample_x.shape}, sample_y shape: {sample_y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define our model as a class\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=1,\n",
    "                    num_layers=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
    "\n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(self.hidden_dim, output_dim)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of lstm_out: [input_size, batch_size, hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both \n",
    "        # have shape (num_layers, batch_size, hidden_dim).\n",
    "        lstm_out, self.hidden = self.lstm(input.view(len(input), self.batch_size, -1))\n",
    "        \n",
    "        # Only take the output from the final timetep\n",
    "        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
    "        y_pred = self.linear(lstm_out[-1].view(self.batch_size, -1))\n",
    "        return y_pred.view(-1)\n",
    "\n",
    "model = LSTM(4, 10, batch_size=1, output_dim=4, num_layers=10)\n",
    "model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "learning_rate = 1e-3\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#####################\n",
    "# Train model\n",
    "#####################\n",
    "\n",
    "num_epochs = 1\n",
    "break_idx = 100\n",
    "hist = np.zeros(num_epochs * len(dataset))\n",
    "\n",
    "for t in range(num_epochs):\n",
    "    # Clear stored gradient\n",
    "    for data_idx, np_data in enumerate(fileset):\n",
    "        \n",
    "        if data_idx == break_idx:\n",
    "            break\n",
    "            \n",
    "        dataset = LSTMLoader(np_data)\n",
    "        data_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "        model.hidden = model.init_hidden()\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "\n",
    "        print(f\"new stock: {data_idx}\")\n",
    "        try:\n",
    "            for i, (x, y) in enumerate(data_loader):\n",
    "                model.zero_grad()\n",
    "\n",
    "                # Initialise hidden state\n",
    "                # Don't do this if you want your LSTM to be stateful\n",
    "\n",
    "\n",
    "                # Forward pass\n",
    "                y_pred = model(x)\n",
    "\n",
    "                loss = loss_fn(y_pred, y)\n",
    "                if t % 100 == 0:\n",
    "                    print(\"Epoch \", t, \"MSE: \", loss.item())    \n",
    "                if t % 1000 == 0:\n",
    "                    print(f'y_pred: {y_pred}, y: {y}')\n",
    "                hist[t] = loss.item()\n",
    "\n",
    "                # Zero out gradient, else they will accumulate between epochs\n",
    "                optimiser.zero_grad()\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # Update parameters\n",
    "                optimiser.step()\n",
    "        except IndexError:\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(y_pred.detach().numpy(), label=\"Preds\")\n",
    "plt.plot(y_train.detach().numpy(), label=\"Data\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(hist, label=\"Training loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix coefficients used so can compare plots with and without noise.\n",
    "c = fixed_ar_coefficients\n",
    "\n",
    "# Generate AR(5) with stable poles, no noise\n",
    "stable_ar = ARData(num_datapoints=50, coeffs=c[5], num_prev=5, noise_var=0)\n",
    "\n",
    "plt.plot(stable_ar.y)\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('x_t')\n",
    "plt.title(\"Stable AR data (no noise)\")\n",
    "# plt.savefig('stable_ar.jpg')\n",
    "plt.show()\n",
    "\n",
    "# Generate AR(5) with stable poles, Gaussian noise\n",
    "stable_ar = ARData(num_datapoints=50, coeffs=c[5], num_prev=5, noise_var=1)\n",
    "\n",
    "plt.plot(stable_ar.y)\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('x_t')\n",
    "plt.title(\"Stable AR data (noise var = 1)\")\n",
    "# plt.savefig('stable_ar_noisy.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
